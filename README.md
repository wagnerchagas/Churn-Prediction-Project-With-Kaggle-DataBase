## ðŸ§  Projeto de PrevisÃ£o de Churn em TelecomunicaÃ§Ãµes

## PT/EN
## EN version at the end of the PT version

**Recursos**:

* **Dashboard Streamlit**: [https://churn-prediction-project-with-kaggle-database-g5q9uswffyie7tzb.streamlit.app/](https://churn-prediction-project-with-kaggle-database-g5q9uswffyie7tzb.streamlit.app/)
* **Imagens**: pasta â€œprints do funcionamentoâ€ com capturas das funcionalidades, em estÃ¡gio inicial e final.

---

## ðŸ“‰ 1. IntroduÃ§Ã£o

Este projeto teve como objetivo prever o churn de clientes de uma operadora de telecomunicaÃ§Ãµes, utilizando o conjunto de dados **Telco Customer Churn** (Kaggle). AtravÃ©s de tÃ©cnicas de aprendizado de mÃ¡quina e anÃ¡lise exploratÃ³ria, concentrei meus esforÃ§os em identificar os principais fatores que influenciam a saÃ­da dos clientes, desenvolver um modelo preditivo eficaz e propor estratÃ©gias de retenÃ§Ã£o baseadas em perfis de risco. AlÃ©m disso, incorporei uma anÃ¡lise de custo-benefÃ­cio para apoiar a tomada de decisÃ£o orientada por dados.

**Origem dos Dados**: Telco Customer Churn (Kaggle)
**TÃ©cnicas Principais**:

1. **Random Forest** â€” escolhi por sua robustez e facilidade de interpretaÃ§Ã£o via importÃ¢ncia de atributos.
2. **SMOTETomek (SMOTE + Tomek Links)** â€” gerei amostras sintÃ©ticas da classe minoritÃ¡ria (SMOTE) e removi pares de pontos ambÃ­guos na fronteira de decisÃ£o (Tomek Links).
3. **Engenharia de Features AvanÃ§ada** â€” criei variÃ¡veis a partir de padrÃµes apontados pela anÃ¡lise de erros (e.g., `tenure_group`, `high_value_flag`, `SeniorContractCombo`, `SupportServicesCount`).
4. **OtimizaÃ§Ã£o de Threshold via Fâ‚‚-Score** â€” priorizei o recall (capturar churners) em relaÃ§Ã£o Ã  precisÃ£o.
5. **PrÃ©-processamento e Pipeline Unificado** â€” utilizei `ColumnTransformer` para imputaÃ§Ã£o e escala das variÃ¡veis numÃ©ricas, e codificaÃ§Ã£o das categÃ³ricas, tudo encadeado em um Ãºnico pipeline sem vazamento de dados.
6. **Hyperparameter Tuning com RandomizedSearchCV** â€” realizei busca aleatÃ³ria em um espaÃ§o de parÃ¢metros (e.g., `n_estimators`, `max_depth`, `class_weight`, `smote__sampling_strategy`) usando `make_scorer(fbeta_score, beta=2)` para otimizar o Fâ‚‚â€‘Score.

---

## ðŸŽ¯ 2. Objetivo do Projeto

Prever churn de clientes com alta sensibilidade (recall) e precisÃ£o suficiente para que a equipe de retenÃ§Ã£o:

1. Concentre esforÃ§os nos casos de maior risco.
2. Minimize desperdÃ­cio de recursos em falsos alarmes.

---

## ðŸ” 3. Entendimento e IniciaÃ§Ã£o

### 3.1 Escolha do Dataset

* **Fonte**: [Telco Customer Churn (Kaggle)](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
* **ConteÃºdo**:

  * Dados demogrÃ¡ficos (gÃªnero, idade, dependentes)
  * Tipo de contrato (`Contract`), mÃ©todo de pagamento
  * Uso de serviÃ§os (TV, internet, suporte tÃ©cnico)
  * MÃ©tricas de consumo (`MonthlyCharges`, `TotalCharges`)
  * Flag `Churn` (â€œYesâ€/â€œNoâ€)

### 3.2 Perguntas de NegÃ³cio

* Quais atributos realmente influenciam a decisÃ£o de churn?
* Como limpar inconsistÃªncias (e.g., `TotalCharges` como texto, espaÃ§os vazios)?
* Em um dataset desbalanceado (\~27% churn), como priorizar a detecÃ§Ã£o de churners sem gerar alarmes falsos em excesso?

### 3.3 Respostas Ã s Perguntas de NegÃ³cio

1. **Atributos que influenciam o churn**
   Concentrei-me em extrair `feature_importances_` do Random Forest e apresentei um grÃ¡fico/tabela com os 10 atributos mais significativos.
2. **Limpeza de inconsistÃªncias**

   * Convertemos `TotalCharges` de string para numÃ©rico:

     ```python
     df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].str.strip().replace('', np.nan), errors='coerce').fillna(0)
     ```
   * Imputei valores faltantes em categÃ³ricas com `'Unknown'` e em numÃ©ricas com a mediana.
   * Removi identificadores (`customerID`) e verifiquei duplicatas.
3. **Tratamento do desbalanceamento**

   * Utilizei SMOTETomek para gerar churners sintÃ©ticos e remover exemplos ambÃ­guos.
   * Otimizei o threshold via Fâ‚‚â€‘Score (Î²=2) para maximizar recall sem Explodir falsos positivos.

---

## ðŸ§¼ 4. Limpeza e PreparaÃ§Ã£o dos Dados

### 4.1 RemoÃ§Ã£o de Identificadores

* Descartei `customerID` por nÃ£o agregar valor preditivo.

### 4.2 ConversÃ£o de `TotalCharges`

* Detectei espaÃ§os e strings vazias:

  ```python
  df['TotalCharges'].sample(10)
  ```
* Apliquei a funÃ§Ã£o de conversÃ£o (descrita acima) para evitar erros no pipeline.

### 4.3 ImputaÃ§Ã£o de Valores Faltantes

* **CategÃ³ricas**: preenchi com `'Unknown'`.
* **NumÃ©ricas**: preenchi com a mediana.
  VerificaÃ§Ã£o final:

```python
print(df.isnull().sum())  # Todos os valores nulos zerados
```

### 4.4 Checagem de Duplicatas e Tipos

```python
print(df.duplicated().sum())  # â†’ 0 duplicatas
print(df.dtypes)             # â†’ tipos adequados
```

Insight: dados bem limpos evitam vazamentos e erros de conversÃ£o no pipeline.

---

## ðŸ“Š 5. Engenharia de Features

### 5.1 Features Base

| Feature                  | Como Criada                                             | PorquÃª                                                                   |
| ------------------------ | ------------------------------------------------------- | ------------------------------------------------------------------------ |
| tenure\_group            | `pd.cut(df.tenure, bins=[0,6,12,24,60,72], labels=[â€¦])` | Captura padrÃµes nÃ£o lineares de risco em diferentes duraÃ§Ãµes do contrato |
| avg\_charge\_per\_tenure | `df['TotalCharges'] / df['tenure'].replace(0,1)`        | Normaliza o gasto total pelo tempo de permanÃªncia                        |
| high\_value\_flag        | `(MonthlyCharges > Q3) & (tenure > median)`             | Detecta clientes de alto valor propensos a churn precoce                 |
| service\_density         | Soma de â€œYesâ€ em colunas de serviÃ§os extras             | Clientes com poucos serviÃ§os extras apresentam churn mais rÃ¡pido         |

### 5.2 Features Derivadas da AnÃ¡lise de Erro

Durante a anÃ¡lise de erros (v2), observei padrÃµes em falsos negativos (FNs) e falsos positivos (FPs):

```python
# False Negatives
fn_mask = (y_test==1) & (y_pred==0)
print(X_test[fn_mask][['Contract','PaymentMethod','MonthlyCharges','tenure']].describe())

# False Positives
fp_mask = (y_test==0) & (y_pred==1)
print(X_test[fp_mask][['Contract','PaymentMethod','MonthlyCharges','tenure']].describe())
```

**PadrÃµes Detectados:**

* **FN** (churn nÃ£o previsto): altas cobranÃ§as iniciais, tenure baixo, contrato mensal.
* **FP** (alarme falso): pagamento automÃ¡tico, contratos anuais, mÃºltiplos serviÃ§os ativos.

**Novas Features Criadas**

| Feature              | Como Criada                                                                      | PorquÃª                                                                 |
| -------------------- | -------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| MonthlyCharges\_log  | `np.log1p(df['MonthlyCharges'])`                                                 | Reduz skew e enfatiza variaÃ§Ãµes relativas em cobranÃ§as altas           |
| SeniorContractCombo  | `(df.SeniorCitizen==1).astype(int) * df.Contract.map({'Month-to-month':1, ...})` | Captura risco elevado de clientes sÃªniores em contratos menos estÃ¡veis |
| SupportServicesCount | Soma de â€œYesâ€ em serviÃ§os de suporte                                             | Clientes sem suporte extra tÃªm +35% de chance de churn                 |

### 5.3 ImportÃ¢ncia dos Atributos

A seguir, o topâ€‘10 de features ordenadas pelo `feature_importances_` do Random Forest:

| Atributo                        | ImportÃ¢ncia (%) |
| ------------------------------- | --------------- |
| Contract\_Month-to-month        | 17.43%          |
| Contract\_Two year              | 10.44%          |
| OnlineSecurity\_No              | 10.11%          |
| TechSupport\_No                 | 8.34%           |
| PaymentMethod\_Electronic check | 6.01%           |
| tenure                          | 5.63%           |
| InternetService\_Fiber optic    | 3.79%           |
| tenure\_group\_0-6              | 3.45%           |
| TotalCharges                    | 3.12%           |
| MonthlyCharges                  | 2.74%           |

---

## âš™ï¸ 6. Pipeline de Machine Learning

### 6.1 SeparaÃ§Ã£o de VariÃ¡veis

```python
X = df.drop('Churn', axis=1)
y = df['Churn'].map({'Yes':1, 'No':0})
```

### 6.2 PrÃ©-processamento com ColumnTransformer

**NumÃ©ricas**:

```python
('num', Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
]), numeric_features)
```

**CategÃ³ricas**:

```python
('cat', Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
]), categorical_features)
```

### 6.3 Balanceamento de Classes com SMOTETomek

```python
from imblearn.combine import SMOTETomek
('smote', SMOTETomek(sampling_strategy=1.0, random_state=42))
```

**Por quÃª?** Gera churners sintÃ©ticos (SMOTE) e remove pontos ambÃ­guos (Tomek Links), melhorando a separabilidade e reduzindo overfitting.

### 6.4 Classificador Random Forest

```python
('classifier', RandomForestClassifier(
    class_weight='balanced',
    random_state=42
))
```

### 6.5 Encadeamento em Pipeline

```python
pipeline = Pipeline([
    ('create_features', FunctionTransformer(create_features, validate=False)),
    ('preprocessor', preprocessor),
    ('smote', SMOTETomek(sampling_strategy=1.0, random_state=42)),
    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))
])
```

BenefÃ­cio: encapsula todo o fluxo, evita vazamento de dados e garante reprodutibilidade.

---

## ðŸŽ¯ 7. Tuning de HiperparÃ¢metros e Fâ‚‚-Score

### 7.1 MotivaÃ§Ã£o

KPI principal: capturar churners (recall) sem gerar custos elevados com falsos positivos.
MÃ©trica escolhida: Fâ‚‚-Score (Î²=2), que penaliza mais fortemente os falsos negativos.

### 7.2 ConfiguraÃ§Ã£o do Scorer

```python
from sklearn.metrics import make_scorer, fbeta_score
f2_scorer = make_scorer(fbeta_score, beta=2)
```

### 7.3 RandomizedSearchCV

```python
search = RandomizedSearchCV(
    pipeline, param_dist, n_iter=40, cv=cv,
    scoring=f2_scorer, verbose=1, n_jobs=-1, random_state=42
)
search.fit(X_train, y_train)
best_model = search.best_estimator_
```

Resultado: recall > 90% na classe churn e precisÃ£o consistentemente melhorada.

---

## ðŸŽ›ï¸ 8. Threshold DinÃ¢mico & ClassificaÃ§Ã£o de Risco

### 8.1 CÃ¡lculo do Threshold Ã“timo

```python
from sklearn.metrics import precision_recall_curve
prec, rec, thr = precision_recall_curve(y_test, y_probs)
f2_scores = (1 + 2**2) * (prec * rec) / (4 * prec + rec)
optimal_threshold = thr[np.nanargmax(f2_scores)]
```

### 8.2 ClassificaÃ§Ã£o de Risco

```python
def classify_risk(score):
    if score >= optimal_threshold + 0.25:
        return 'High'
    elif score >= optimal_threshold:
        return 'Moderate'
    else:
        return 'Low'
```

### 8.3 CÃ¡lculo de Custo por PrevisÃ£o

```python
df_result['PredictionType'] = np.select(
    [
        (df_result['True']==1) & (df_result['Pred']==0),
        (df_result['True']==0) & (df_result['Pred']==1)
    ],
    ['FN', 'FP'],
    default='TN/TP'
)
df_result['Cost'] = np.select(
    [df_result['PredictionType']=='FP', df_result['PredictionType']=='FN'],
    [100, 2000],
    default=0
)
```

---

## ðŸ“ˆ 9. Resultados Finais

**Optimized Classification Report:**

| Classe           | Precision | Recall | F1-score | Support |
| ---------------- | --------- | ------ | -------- | ------- |
| 0                | 0.96      | 0.57   | 0.72     | 1053    |
| 1                | 0.42      | 0.93   | 0.58     | 352     |
| **accuracy**     |           |        | 0.66     | 1405    |
| **macro avg**    | 0.69      | 0.75   | 0.65     | 1405    |
| **weighted avg** | 0.82      | 0.66   | 0.68     | 1405    |

**Optimal Threshold**: 0.45
**Recall (Churn)**: 93% â€” captura quase todos os churners.
**PrecisÃ£o (Churn)**: 42% â€” aceitÃ¡vel dado o alto custo de perder um churner.

---

## ðŸ’¡ 10. Comparativo de VersÃµes

| VersÃ£o | Acc  | Prec | Recall | F1   | F2   | AUC-ROC | Principais MudanÃ§as             |
| ------ | ---- | ---- | ------ | ---- | ---- | ------- | ------------------------------- |
| v1     | 0.79 | 0.71 | 0.45   | 0.55 | 0.49 | 0.80    | Modelo baseline                 |
| v2     | 0.83 | 0.61 | 0.90   | 0.73 | 0.75 | 0.83    | Threshold otimizado (PR curve)  |
| v3     | 0.83 | 0.42 | 0.93   | 0.58 | 0.66 | 0.83    | Ajustes de features e F2-tuning |

**Nota**: AUC-ROC â‰ˆ 0.83 confirma bom poder discriminativo, nÃ£o aleatoriedade.

---

## ðŸ“Œ 11. ConclusÃ£o Operacional e PrÃ³ximos Passos

Este modelo atinge alto recall com custo operacional baixo, graÃ§as ao uso de e-mails e mensagens automatizadas.

* **FNs** ficaram abaixo de 7%, evitando grandes perdas.
* **FPs** geram custo mÃ­nimo e criam oportunidades de engajamento.
* A escolha do Fâ‚‚-Score reforÃ§a a prioridade de capturar churners.

### 11.1 EstratÃ©gias de AÃ§Ã£o para Clientes de Alto Risco

| Segmento   | CritÃ©rio                | NÂº de Clientes | AÃ§Ã£o                        |
| ---------- | ----------------------- | -------------- | --------------------------- |
| CrÃ­tico    | Prob. Churn â‰¥ 80%       | 120            | Atendimento humano imediato |
| Alto Risco | 60% â‰¤ Prob. Churn < 80% | 300            | Campanhas personalizadas    |
| Monitorar  | 45% â‰¤ Prob. Churn < 60% | 500            | Engajamento preventivo      |

### 11.2 Kit de AÃ§Ãµes por Segmento

* **ðŸ”´ CrÃ­tico (Prob. â‰¥ 80%)**: contato humano em atÃ© 24h, oferta VIP (desconto + upgrade), visita tÃ©cnica preventiva. *Meta: â‰¥ 65% de taxa de conversÃ£o.*
* **ðŸŸ  Alto Risco (60%â€“80%)**: e-mail segmentado, reforÃ§o via SMS. *Meta: â‰¥ 45% de abertura/interaÃ§Ã£o.*
* **ðŸŸ¢ Monitorar (45%â€“60%)**: comunicaÃ§Ã£o automÃ¡tica (e-mail/SMS genÃ©rico), promoÃ§Ãµes leves (crÃ©dito extra, gamificaÃ§Ã£o). *Meta: â‰¥ 25% de resposta.*

### 11.3 CÃ¡lculo de Custo-BenefÃ­cio e ROI

| Item                 | Custo UnitÃ¡rio | RetenÃ§Ã£o Estimada |
| -------------------- | -------------- | ----------------- |
| LigaÃ§Ã£o VIP          | R\$ 150        | 70%               |
| E-mail Personalizado | R\$ 10         | 40%               |
| SMS                  | R\$ 2          | 15%               |

```text
custo_total = (120 * 150) + (300 * 10) + (500 * 2) = R$ 24.000
receita_preservada = (120*0.7 + 300*0.4 + 500*0.15) * 2500 = R$ 1.162.500
ROI = (1.162.500 - 24.000) / 24.000 â‰ˆ 4743%
```

### 11.4 Fluxo de GovernanÃ§a

* **DiÃ¡rio**: atualizar lista de clientes crÃ­ticos Ã s 8h.
* **Semanal**: reuniÃ£o de anÃ¡lise de conversÃµes com equipe de CX.
* **Mensal**: ajustar limites de probabilidade conforme capacidade e testar novas mensagens/ofertas com A/B testing.

### 11.5 Alertas Proativos (Exemplo)

```python
if prob_churn >= 0.8 and contract == "Monthly" and support_tickets >= 3:
    enviar_para_fila("URGENTE", cliente_id)
elif prob_churn >= 0.6 and internet_service == "Fiber":
    oferecer_upgrade_gratis(cliente_id)
```

### 11.6 Resultados Esperados

* ReduÃ§Ã£o de 25â€“30% no churn nos segmentos CrÃ­tico e Alto Risco.
* ROI de 10:1 para cada real investido em retenÃ§Ã£o.
* Aumento de 15% no NPS devido a aÃ§Ãµes personalizadas.


# ENGLISH Version

## ðŸ§  Telecom Customer Churn Prediction Project

**Resources**:

* **Streamlit Dashboard**: [https://churn-prediction-project-with-kaggle-database-g5q9uswffyie7tzb.streamlit.app/](https://churn-prediction-project-with-kaggle-database-g5q9uswffyie7tzb.streamlit.app/)
* **Images**: folder â€œprints do funcionamentoâ€ containing screenshots of features in initial and final stages.

---

## ðŸ“‰ 1. Introduction

This project aimed to predict customer churn for a telecommunications provider using the **Telco Customer Churn** dataset (Kaggle). Through machine learning techniques and exploratory analysis, I focused on identifying the key factors driving customer attrition, developing an effective predictive model, and proposing targeted retention strategies based on risk profiles. Additionally, I incorporated a costâ€“benefit analysis to support data-driven decision making.

**Data Source**: Telco Customer Churn (Kaggle)
**Key Techniques**:

1. **Random Forest** â€” chosen for its robustness and ease of interpretability via feature importance.
2. **SMOTETomek (SMOTE + Tomek Links)** â€” synthetic minority oversampling (SMOTE) combined with Tomek Links to remove ambiguous boundary samples.
3. **Advanced Feature Engineering** â€” created new variables informed by error analysis patterns (e.g., `tenure_group`, `high_value_flag`, `SeniorContractCombo`, `SupportServicesCount`).
4. **Threshold Optimization via Fâ‚‚-Score** â€” prioritized recall (capturing churners) over precision.
5. **Preprocessing & Unified Pipeline** â€” used `ColumnTransformer` for numeric imputation/scaling and categorical encoding, all within a single pipeline to prevent data leakage.
6. **Hyperparameter Tuning with RandomizedSearchCV** â€” performed random search over hyperparameters (e.g., `n_estimators`, `max_depth`, `class_weight`, `smote__sampling_strategy`) using `make_scorer(fbeta_score, beta=2)` to optimize the Fâ‚‚-Score.

---

## ðŸŽ¯ 2. Project Objective

Predict customer churn with high sensitivity (recall) and sufficient precision so that the retention team can:

1. Focus efforts on the highest-risk cases.
2. Minimize resource waste on false alarms.

---

## ðŸ” 3. Understanding & Initiation

### 3.1 Dataset Selection

* **Source**: [Telco Customer Churn (Kaggle)](https://www.kaggle.com/datasets/blastchar/telco-customer-churn)
* **Contents**:

  * Demographics (gender, age, dependents)
  * Contract type (`Contract`), payment method
  * Service usage (TV, internet, tech support)
  * Consumption metrics (`MonthlyCharges`, `TotalCharges`)
  * `Churn` flag (â€œYesâ€/â€œNoâ€)

### 3.2 Business Questions

* Which attributes truly influence churn decision?
* How to clean inconsistencies (e.g., `TotalCharges` stored as text, blank spaces)?
* With an imbalanced dataset (\~27% churn), how to prioritize churn detection without generating excessive false alarms?

### 3.3 Answers to Business Questions

1. **Key churn-driving features**
   I extracted `feature_importances_` from the Random Forest and presented a chart/table of the top 10 most significant features.
2. **Data cleaning approach**

   * Converted `TotalCharges` from string to numeric:

     ```python
     df['TotalCharges'] = pd.to_numeric(df['TotalCharges'].str.strip().replace('', np.nan), errors='coerce').fillna(0)
     ```
   * Imputed missing values: `'Unknown'` for categoricals and median for numerics.
   * Dropped identifier `customerID` and checked for duplicates.
3. **Handling class imbalance**

   * Applied SMOTETomek to generate synthetic churn examples and remove ambiguous samples.
   * Optimized classification threshold via Fâ‚‚-Score (Î²=2) to maximize recall without drastically increasing false positives.

---

## ðŸ§¼ 4. Data Cleaning & Preparation

### 4.1 Dropping Identifiers

* Removed `customerID` as it provided no predictive value.

### 4.2 Converting `TotalCharges`

* Detected blank spaces and empty strings:

  ```python
  df['TotalCharges'].sample(10)
  ```
* Applied conversion function to avoid pipeline errors.

### 4.3 Missing Value Imputation

* **Categorical**: filled with `'Unknown'`.
* **Numeric**: filled with median.
  Final check:

```python
print(df.isnull().sum())  # No remaining nulls
```

### 4.4 Duplicate & Type Check

```python
print(df.duplicated().sum())  # â†’ 0 duplicates
print(df.dtypes)             # â†’ correct dtypes
```

Insight: well-cleaned data prevents leaks and conversion errors.

---

## ðŸ“Š 5. Feature Engineering

### 5.1 Base Features

| Feature                  | Creation Method                                         | Rationale                                                   |
| ------------------------ | ------------------------------------------------------- | ----------------------------------------------------------- |
| tenure\_group            | `pd.cut(df.tenure, bins=[0,6,12,24,60,72], labels=[â€¦])` | Captures non-linear risk patterns across contract durations |
| avg\_charge\_per\_tenure | `df['TotalCharges'] / df['tenure'].replace(0,1)`        | Normalizes spending by tenure duration                      |
| high\_value\_flag        | `(MonthlyCharges > Q3) & (tenure > median)`             | Identifies high-value customers prone to early churn        |
| service\_density         | Sum of â€œYesâ€ across additional service columns          | Low-service customers churn faster                          |

### 5.2 Error Analysisâ€“Driven Features

During error analysis (v2), I observed patterns in false negatives (FNs) and false positives (FPs):

```python
# False Negatives
fn_mask = (y_test==1) & (y_pred==0)
print(X_test[fn_mask][['Contract','PaymentMethod','MonthlyCharges','tenure']].describe())

# False Positives
fp_mask = (y_test==0) & (y_pred==1)
print(X_test[fp_mask][['Contract','PaymentMethod','MonthlyCharges','tenure']].describe())
```

**Patterns Detected:**

* **FN**: high initial charges, low tenure, month-to-month contracts.
* **FP**: automatic payments, annual contracts, multiple active services.

**New Features Created**

| Feature              | Creation Method                                              | Rationale                                                           |
| -------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------- |
| MonthlyCharges\_log  | `np.log1p(df['MonthlyCharges'])`                             | Reduces skew, highlights relative variations in high charges        |
| SeniorContractCombo  | `(df.SeniorCitizen==1).astype(int) * df.Contract.map({...})` | Captures elevated risk for senior customers with unstable contracts |
| SupportServicesCount | Sum of â€œYesâ€ across support service columns                  | Customers without extra support services have +35% churn risk       |

### 5.3 Feature Importance

Top 10 features sorted by `feature_importances_` from the Random Forest:

| Feature                         | Importance (%) |
| ------------------------------- | -------------- |
| Contract\_Month-to-month        | 17.43%         |
| Contract\_Two year              | 10.44%         |
| OnlineSecurity\_No              | 10.11%         |
| TechSupport\_No                 | 8.34%          |
| PaymentMethod\_Electronic check | 6.01%          |
| tenure                          | 5.63%          |
| InternetService\_Fiber optic    | 3.79%          |
| tenure\_group\_0-6              | 3.45%          |
| TotalCharges                    | 3.12%          |
| MonthlyCharges                  | 2.74%          |

---

## âš™ï¸ 6. Machine Learning Pipeline

### 6.1 Trainâ€“Test Split & Label Encoding

```python
X = df.drop('Churn', axis=1)
y = df['Churn'].map({'Yes':1, 'No':0})
```

### 6.2 Preprocessing via ColumnTransformer

**Numeric Pipeline**:

```python
('num', Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
]), numeric_features)
```

**Categorical Pipeline**:

```python
('cat', Pipeline([
    ('imputer', SimpleImputer(strategy='constant', fill_value='Unknown')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
]), categorical_features)
```

### 6.3 Class Imbalance Handling with SMOTETomek

```python
from imblearn.combine import SMOTETomek
('smote', SMOTETomek(sampling_strategy=1.0, random_state=42))
```

**Why?** Generates synthetic churn samples and removes boundary noise, improving separability and reducing overfitting.

### 6.4 Random Forest Classifier

```python
('classifier', RandomForestClassifier(
    class_weight='balanced',
    random_state=42
))
```

### 6.5 Full Pipeline Assembly

```python
pipeline = Pipeline([
    ('create_features', FunctionTransformer(create_features, validate=False)),
    ('preprocessor', preprocessor),
    ('smote', SMOTETomek(sampling_strategy=1.0, random_state=42)),
    ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))
])
```

Benefit: encapsulates the entire workflow, prevents data leakage, and ensures reproducibility.

---

## ðŸŽ¯ 7. Hyperparameter Tuning & Fâ‚‚-Score

### 7.1 Motivation

Primary KPI: capture churners (recall) while controlling false-positive costs.
Chosen metric: Fâ‚‚-Score (Î²=2), which penalizes false negatives more heavily.

### 7.2 Scorer Configuration

```python
from sklearn.metrics import make_scorer, fbeta_score
f2_scorer = make_scorer(fbeta_score, beta=2)
```

### 7.3 RandomizedSearchCV

```python
search = RandomizedSearchCV(
    pipeline, param_dist, n_iter=40, cv=cv,
    scoring=f2_scorer, verbose=1, n_jobs=-1, random_state=42
)
search.fit(X_train, y_train)
best_model = search.best_estimator_
```

Result: recall > 90% for churn class and consistently improved precision.

---

## ðŸŽ›ï¸ 8. Dynamic Threshold & Risk Classification

### 8.1 Optimal Threshold Calculation

```python
from sklearn.metrics import precision_recall_curve
prec, rec, thr = precision_recall_curve(y_test, y_probs)
f2_scores = (1 + 2**2) * (prec * rec) / (4 * prec + rec)
optimal_threshold = thr[np.nanargmax(f2_scores)]
```

### 8.2 Risk Classification Function

```python
def classify_risk(score):
    if score >= optimal_threshold + 0.25:
        return 'High'
    elif score >= optimal_threshold:
        return 'Moderate'
    else:
        return 'Low'
```

### 8.3 Cost-per-Prediction Calculation

```python
df_result['PredictionType'] = np.select(
    [
        (df_result['True']==1) & (df_result['Pred']==0),
        (df_result['True']==0) & (df_result['Pred']==1)
    ],
    ['FN', 'FP'],
    default='TN/TP'
)
df_result['Cost'] = np.select(
    [df_result['PredictionType']=='FP', df_result['PredictionType']=='FN'],
    [100, 2000],
    default=0
)
```

---

## ðŸ“ˆ 9. Final Results

**Optimized Classification Report:**

| Class            | Precision | Recall | F1-score | Support |
| ---------------- | --------- | ------ | -------- | ------- |
| 0                | 0.96      | 0.57   | 0.72     | 1053    |
| 1                | 0.42      | 0.93   | 0.58     | 352     |
| **accuracy**     |           |        | 0.66     | 1405    |
| **macro avg**    | 0.69      | 0.75   | 0.65     | 1405    |
| **weighted avg** | 0.82      | 0.66   | 0.68     | 1405    |

**Optimal Threshold**: 0.45
**Recall (Churn)**: 93% â€” captures almost all churners.
**Precision (Churn)**: 42% â€” acceptable given the high cost of losing a churner.

---

## ðŸ’¡ 10. Version Comparison

| Version | Acc  | Prec | Recall | F1   | F2   | AUC-ROC | Key Changes                     |
| ------- | ---- | ---- | ------ | ---- | ---- | ------- | ------------------------------- |
| v1      | 0.79 | 0.71 | 0.45   | 0.55 | 0.49 | 0.80    | Baseline model                  |
| v2      | 0.83 | 0.61 | 0.90   | 0.73 | 0.75 | 0.83    | Optimized threshold (PR curve)  |
| v3      | 0.83 | 0.42 | 0.93   | 0.58 | 0.66 | 0.83    | Feature adjustments & Fâ‚‚ tuning |

**Note**: AUC-ROC â‰ˆ 0.83 confirms strong discriminative power, not randomness.

---

## ðŸ“Œ 11. Operational Conclusion & Next Steps

This model achieves high recall with low operational cost thanks to automated emails and messages.

* **FNs** remained below 7%, preventing major losses.
* **FPs** incur minimal cost and create engagement opportunities.
* Using Fâ‚‚-Score underscores the priority of capturing churners.

### 11.1 High-Risk Customer Actions

| Segment   | Criterion              | # of Customers | Action                   |
| --------- | ---------------------- | -------------- | ------------------------ |
| Critical  | Churn Prob â‰¥ 80%       | 120            | Immediate human outreach |
| High Risk | 60% â‰¤ Churn Prob < 80% | 300            | Targeted campaigns       |
| Monitor   | 45% â‰¤ Churn Prob < 60% | 500            | Preventive engagement    |

### 11.2 Action Kit by Segment

* **ðŸ”´ Critical (Prob â‰¥ 80%)**: human contact within 24h, VIP offer (discount + upgrade), preventive technical visit. *Goal: â‰¥ 65% conversion.*
* **ðŸŸ  High Risk (60%â€“80%)**: segmented email, SMS follow-up. *Goal: â‰¥ 45% open/interaction.*
* **ðŸŸ¢ Monitor (45%â€“60%)**: automated email/SMS, light promotions (credit, gamification). *Goal: â‰¥ 25% response.*

### 11.3 Costâ€“Benefit & ROI Calculation

| Item               | Unit Cost | Estimated Retention |
| ------------------ | --------- | ------------------- |
| VIP Call           | R\$ 150   | 70%                 |
| Personalized Email | R\$ 10    | 40%                 |
| SMS                | R\$ 2     | 15%                 |

```text
cost_total = (120 * 150) + (300 * 10) + (500 * 2) = R$ 24,000
retained_revenue = (120*0.7 + 300*0.4 + 500*0.15) * 2500 = R$ 1,162,500
ROI = (1,162,500 - 24,000) / 24,000 â‰ˆ 4743%
```

### 11.4 Governance Workflow

* **Daily**: update critical customer list at 8am.
* **Weekly**: review conversion metrics with CX team.
* **Monthly**: adjust probability thresholds and A/B test new offers/messages.

### 11.5 Proactive Alerts (Example)

```python
if prob_churn >= 0.8 and contract == "Monthly" and support_tickets >= 3:
    enqueue_urgent_alert(customer_id)
elif prob_churn >= 0.6 and internet_service == "Fiber":
    offer_free_upgrade(customer_id)
```

### 11.6 Expected Outcomes

* 25â€“30% churn reduction in Critical and High Risk segments.
* 10:1 ROI on each real invested in retention.
* 15% increase in NPS due to personalized interventions.


